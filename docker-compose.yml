version: '3.8'

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    # Ollama by default listens on 11434
    ports:
      - "11434:11434" # Expose Ollama's port if you want to access it directly from your host
    volumes:
      - ollama_data:/root/.ollama # Persist Ollama models and data
    # Optional: If you have a GPU and want Ollama to use it (requires NVIDIA Container Toolkit)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: always

  langchain_app:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: langchain_rag_server
    ports:
      - "8000:8000" # Expose FastAPI's port to your host
    volumes:
      - ./data:/app/data # Mount your data directory for RAG
      - chroma_db_data:/app/chroma_db # Persist ChromaDB data if used
    environment:
      # Pass the Ollama service URL to the FastAPI app
      # 'ollama' is the service name, and 11434 is its default port
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL_NAME: phi3:mini # Or your preferred LLM, e.g., llama2, gemma
      EMBEDDING_MODEL_NAME: nomic-embed-text # Or a HuggingFace model like BAAI/bge-small-en-v1.5
    depends_on:
      - ollama # Ensure Ollama starts before your FastAPI app
    restart: always

volumes:
  ollama_data: # Define the named volume for Ollama data persistence
  chroma_db_data: # Define the named volume for Chrome data persistence (if needed)